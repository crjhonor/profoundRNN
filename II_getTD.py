# # Loading Dataset into a DataLoader ----------------------------------------------------------------------------------
import numpy as np
import pandas as pd
from sklearn.impute import KNNImputer
import math

# Read dataset
TD_all_dataset = pd.read_csv('/home/crjLambda/PRO80/DEEPLEARN/TD_All.csv')
indX = TD_all_dataset.columns.values
indX[0] = 'DATE'
TD_all_dataset.columns = indX

# Also read the indexes
TD_indexes = pd.read_csv('/home/crjLambda/PRO80/DailyTDs/ref_TD.csv')
TD_yields_indexes = pd.read_csv('/home/crjLambda/PRO80/DailyTDs/ref_yields.csv')
TD_Currency_indexes = pd.read_csv('/home/crjLambda/PRO80/DailyTDs/ref_Currency.csv')

# And generate wanted dataset
indexesAll = TD_indexes.join(TD_Currency_indexes, rsuffix='_Currency')
indexesAll = indexesAll.join(TD_yields_indexes, rsuffix='_yields')
indexesAll_ind = indexesAll.iloc[0, ]
"""
Several newly opened contracts have to be eliminated as it contains too much NAs due to much shorter trading periods.
These includes: PK0, LH0, PG0, SA0, EB0, EG0 and SCM.

indexesAll_ind.index = indexesAll_ind.values
indexesAll_ind = indexesAll_ind.drop(['PK0', 'LH0', 'PG0', 'SA0', 'EB0', 'EG0', 'SCM', 'LUM', 'UR0'])
"""

# Dataset of Close
indX = ['DATE']
for ind in indexesAll_ind:
    indX.append(ind+'Close')
datasetClose = TD_all_dataset[indX]
# Dataset of hodrick prescott filter's trend product
indX = ['DATE']
for ind in indexesAll_ind:
    indX.append(ind+'_hpft')
datasetTrend = TD_all_dataset[indX]
# Dataset of hodrick prescott filter's cycle product
indX = ['DATE']
for ind in indexesAll_ind:
    indX.append(ind+'_hpfc')
datasetCycle = TD_all_dataset[indX]

"""
After generating desired datasets, NAs have to be taken care of. There is one major problem with the data which 
generated by the lagging update of the yields. And I am planing to using a 10 days window to impute the NAs within the 
yields.
"""
# Close dataset
datasetClose_upperPart = datasetClose.iloc[:-10]
datasetClose_lowerPart = datasetClose.iloc[-10:]
# Drop NAs of the upperPart
datasetClose_upperPart_dropna = datasetClose_upperPart.dropna(axis=0)
# Impute the lowerPart NAs
imputed_data_DATE = datasetClose_lowerPart['DATE']
imputed_data_DATE = pd.DataFrame(imputed_data_DATE)
imputed_data_noDATE = datasetClose_lowerPart.drop(columns=['DATE'])
imr = KNNImputer(n_neighbors=2, weights='uniform')
imr = imr.fit(imputed_data_noDATE.values)
imputed_data = imr.transform(imputed_data_noDATE.values)
imputed_data_noDATE = pd.DataFrame(imputed_data, columns=imputed_data_noDATE.columns)
imputed_data_noDATE.index = imputed_data_DATE.index
datasetClose_lowerPart_imputedna = imputed_data_DATE.join(imputed_data_noDATE)
datasetClose = datasetClose_upperPart_dropna.append(datasetClose_lowerPart_imputedna)
X_predict_DATE = pd.to_datetime(datasetClose_lowerPart['DATE'].tail(1).values).date

"""
Generate the features using multiple classes.
"""
def generate_logr(dataset, isDATE=True):
    if isDATE:
        dataset_DATE = dataset['DATE']
        dataset_noDATE = dataset.drop(columns=['DATE'])
    else:
        dataset_noDATE = dataset

    dataset_noDATE_pct_change = dataset_noDATE.pct_change(periods=1)
    dataset_noDATE_pct_change = dataset_noDATE_pct_change.iloc[1:]
    dataset_noDATE_logr = dataset_noDATE_pct_change.applymap(lambda x: np.log(x + 1))

    if isDATE:
        dataset_DATE = pd.DataFrame(dataset_DATE.iloc[1:])
        returnDataset = dataset_DATE.join(dataset_noDATE_logr)
    else:
        returnDataset = dataset_noDATE_logr
    return returnDataset


def generate_MultiClassesLabel(dataset, classes=4, isDATE=True):
    """
    function to transform continuous data into discrete data
    :param classes:
    :param isDATE:
    :return:
    """
    assert classes > 2, "classes must > 2 "
    if isDATE:
        dataset_DATE = dataset['DATE']
        dataset_noDATE = dataset.drop(columns=['DATE'])
    else:
        dataset_noDATE = dataset

    # GR10_yryClose and INA_10yryClose have some unsuitable data.
    dataset_noDATE = dataset_noDATE.drop(columns=['GR_10yryClose', 'INA_10yryClose'])

    def to_classes(x, classes=4):
        """
        Let's make it more precisely, that log return are devided into n classes provided as the parameter. And the
        classes should begin from 0 and to n-1, they will represent the range from most negative to most positive
        number. In return, both the transformed dataset and the classesTable are returned.
        I have to save for numbers for special use in classes, they are:
        0, padding number;
        1, not used;
        2, BOS;
        3, EOS;
        """
        x_dvi = 2 * x.std() / (classes - 2)
        x_mean = x.mean()
        x_ceil = np.array([math.ceil(m / x_dvi) for m in x])
        x_range = np.array([m for m in range(-int(classes / 2 - 1), int(classes / 2 + 1))])
        # Adjust the tail
        for i in range(len(x_ceil)):
            if x_ceil[i] < x_range.min():
                x_ceil[i] = x_range.min()
            elif x_ceil[i] > x_range.max():
                x_ceil[i] = x_range.max()
        # Now adjust more
        x_ceil_min = x_ceil.min()
        x_ceil = x_ceil + abs(x_ceil_min) + 4
        # Generate the classTable for returning.
        cT_range = np.arange(4, (classes + 4), 1)
        cT_shift = np.arange(x_mean - ((classes - 2) / 2) * abs(x_dvi),
                             x_mean + ((classes - 2) / 2 + 1) * abs(x_dvi),
                             x_dvi)
        cT_label = []
        for i in range(len(cT_range)):
            if i == 0:
                cT_label.append(''.join(["x < ", str(round(cT_shift[i], ndigits=4))]))
            elif i == (len(cT_range) - 1):
                cT_label.append(''.join([str(round(cT_shift[i - 1], ndigits=4)), ' <= x']))
            else:
                cT_label.append(''.join([str(round(cT_shift[i - 1], ndigits=4)), ' <= x < ',
                                         str(round(cT_shift[i], ndigits=4))]))
        cT_range = np.append(np.array([1, 2, 3]), cT_range)
        cT_label = ['not used', 'BOS', 'EOS'] + cT_label
        classesTable = pd.DataFrame({
            "class": cT_range,
            "stand for": cT_label
        })
        classesTable.index = classesTable['class']
        return x_ceil, classesTable

    for col in dataset_noDATE.columns:
        x_array = dataset_noDATE[col].values
        x_classes, _ = to_classes(x_array, classes=classes)
        dataset_noDATE[col] = x_classes

    if isDATE:
        dataset_DATE = pd.DataFrame(dataset_DATE.iloc[1:])
        returnDataset = dataset_DATE.join(dataset_noDATE)
    else:
        returnDataset = dataset_noDATE
    return returnDataset

def getDataclose(num_classes=4):
    dataClose_logr = generate_logr(datasetClose)
    dataClose_multiclasses = generate_MultiClassesLabel(dataset=dataClose_logr, classes=num_classes)
    dataClose_multiclasses.index = pd.to_datetime(dataClose_multiclasses['DATE'], format="%Y-%m-%d")
    return dataClose_multiclasses

# getDataclose(1000)